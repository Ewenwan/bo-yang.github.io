
<!DOCTYPE HTML>
<html lang="en-US" xmlns="http://www.w3.org/1999/xhtml" xmlns:wb="http://open.weibo.com/wb">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Action Recognition with Fisher Vectors | Bo's Blog</title>
  
  <meta name="author" content="Bo Yang" />
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <!--
  <link href="/assets/themes/clear/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  -->

  <!-- Bootstrap -->
  <link href="/assets/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="/assets/bootstrap/css/bootstrap-theme.min.css" rel="stylesheet">

  <!-- font-awesome -->
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">

  <!-- Google Prettify -->
  <link href="/assets/google-code-prettify/desert.css" rel="stylesheet" type="text/css" media="all">
  <!-- Google Pretty end -->

  <link href="/assets/style/blog.css" rel="stylesheet">
  
  <script src="/assets/bootstrap/js/jquery-1.10.2.js"></script>
  <script src="/assets/bootstrap/js/bootstrap.min.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>

   <!-- navigation bar -->
	<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
				<a class="navbar-brand" href="/"><b>bo-yang</b></a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					<li><a href="/archive.html"><b>Archive</b></a>
					</li>
					<li><a href="/tags.html"><b>Tags</b></a>
                    </li>
                    <li><a href="/about.html"><b>About</b></a>
                    </li>
                    <li><a href="/messages.html"><b>Messages</b></a>
                    </li>
				</ul>
				<ul class="nav navbar-nav pull-right">
		          <li>
		            <form class="navbar-form navbar-search" method="get" action="http://www.google.com/search" target="google_window">
		              <input id="g_search" type="text" class="search-query" placeholder="Search..." name="q" />
		              <input type="submit" name="btnG" style="display:none" id="searchsubmit" value="Search" />
		              <input type="hidden" name="ie" value="UTF-8" />
		              <input type="hidden" name="oe" value="UTF-8" />
		              <input type="hidden" name="hl" value="en-US" />
		              <input type="hidden" name="domains" value="http://bo-yang.github.io/" />
					  <input type="hidden" name="sitesearch" value="http://bo-yang.github.io/" />
					  <button type="submit" class="button button-rounded button-flat-blue">Go</button>
		            </form>
		          </li>
		        </ul>
            </div>
			<!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

  <div class="container">
    <div class="row" id="content-row">
      
<div class="page-header">
	<h1>Action Recognition with Fisher Vectors</h1>
	<p>
		<span class="glyphicon glyphicon-time"></span> Posted at &nbsp; 2014-04-30 &nbsp; &nbsp; by  <a href="/about.html"><strong>Bo Yang</strong></a> &nbsp; &nbsp; <span class="glyphicon glyphicon-tag"></span><a href="/tags.html#Computer+Vision" rel="nofollow">Computer Vision</a>, <span class="glyphicon glyphicon-tag"></span><a href="/tags.html#Machine+Learning" rel="nofollow">Machine Learning</a>
	</p>
</div>

<div class="col-lg-10">
	<div class="post paper">
		<p>This is a summary of doing human action recognition using Fisher Vector with (Improved) Dense Trjectory Features(DTF, http://lear.inrialpes.fr/~wang/improved_trajectories) and STIP features(http://crcv.ucf.edu/ICCV13-Action-Workshop/download.html) on UCF 101 dataset(<a href="http://crcv.ucf.edu/data/UCF101.php">http://crcv.ucf.edu/data/UCF101.php</a>). In the STIP features, two low-level visual features HOG and HOF are integrated, with dimensions 72 and 90 respectively. The (improved) DTF employ more features(TR, HOG, HOF and MBHx/MBHy) with longer dimensions. </p>

<p>You can find my Matlab code from <a href="https://github.com/bo-yang">my GitHub Channel</a>:</p>

<ul>
  <li><a href="https://github.com/bo-yang/dtf_fisher">DTF + Fisher Vector code</a></li>
  <li><a href="https://github.com/bo-yang/stip_fisher">STIP + Fisher Vector code</a></li>
</ul>

<h3 id="dense-trajectory-features">Dense Trajectory Features</h3>

<p>For some details of DTF, please refer to <a href="http://bo-yang.github.io/2014/01/10/dense-trajectory-notes/">my previous post</a>.</p>

<h3 id="pipeline">Pipeline</h3>

<p>The pipeline of integrating DTF/STIP features and Fisher vectors is shown in Figure 1. The first step is subsampling a fixed number of STIP/DTF features(in my implementation, 1000) from each
video clip in training list, which will be used to do PCA and train
Gaussian Mixture Models(GMMs).</p>

<p>After getting the PCA coefficients and GMM parameters, treat UCF 101 video clips
action by action. For each action, first load all train videos in this
action(positive videos), and then randomly load the same number of video
clips not in this action(negative videos). All of the loaded videos are
multiplied with the saved PCA coefficients in order to reduce dimensions
and rotate matrices. Fisher vectors are computed for each loaded video
clip. Finally a binary SVM model is trained with both the positive and negative
Fisher vectors.</p>

<p>When dealing with the test videos, similar process is adopted. The only
difference is that the Fisher vectors are used for SVM classification,
which is based on the SVM model trained with training videos.</p>

<p><img src="/assets/images/fisher_pipeline.png" alt="Figure 1. Pipeline of UCF101 action recognition using Fisher
vector." />.</p>

<p>To well utilize the STIP or DTF features, features(HOG, HOF, MBH, etc.) are treated
separately and they are only combined(simple concatenation) after computing Fisher
vectors before linear SVM classification.</p>

<h3 id="pre-processing">Pre-processing</h3>

<h4 id="stip-features">STIP Features</h4>

<p>The offcial STIP features are stored in class, which means that all the STIP
info of all video clips in each class are mixed together in a file. To
extract STIP features for each video, I wrote a script(mk_stip_data)
to separate STIP features for each video clip. And all the following
operations are based on each video clip.</p>

<h4 id="dtf-features">DTF Features</h4>

<p>Since the DTF features are “dense”(which means a lot of data), it took me 4~5 days to exact the (improved) DTF features of UCF 101 clips with the dedault parameters on a modern Linux desktop(I used 10 threads for extraction in paralle). The installation of DTF tools was also a very tricky task.</p>

<p>To save space, all the DTF features were compressed. For UCF101, it would cost about 500GB after zip compression. And the required space would doubled if not compressing. If you don’t want to save the DTF features, you can call the DTF tools in Matlab and discard the extracted features.</p>

<h3 id="fisher-vector">Fisher Vector</h3>

<p>The Fisher Vector (FV) representation of visual features is an extension of the popular bag-of-visual words (BOV)[1]. Both of them are based on an intermediate representation, the visual vocabulary built in the low level feature space. A probability density function (in most cases a Gaussian Mixture Model) is used to model the visual vocabulary, and we can compute the gradient of the log likelihood with respect to the parameters of the model to represent an image or video. The Fisher Vector is the concatenation of these partial derivatives and describes in which direction the parameters of the model should be modified to best fit the data. This representation has the advantage to give similar or even better classification performance than BOV obtained with supervised visual vocabularies.  </p>

<p>Following is the algorithm of computing Fisher vectors from features(actually I implemented this algorithm in Matlab, and if you are interested, please refer <a href="https://github.com/bo-yang/stip_fisher/blob/master/fisher_encode.m">here</a>):</p>

<p><img src="/assets/images/fisher_vector_algorithm.png" alt="Figure 2. Algorithm of computing Fisher vectors." />.</p>

<p>During the subsampling of STIP features, I randomly chose 1000 HOG or
HOF features from each training video clip. For some videos, if the
total number of features were less than 1000, I would use all of their
features. All the subsampled features are square rooted after L1
normalization.</p>

<p>After that, the dimensions of the subsampled features were reduced to
half of their original dimensions by doing PCA. At this step, the
coefficients of PCA were recorded, which would be used in later. The
GMMs were trained with the half-sized features, and the parameters of
GMMs(i.e. weight, mean and covariance) were stored for the following
process. In my program, the GMM code implemented by Oxford Visual
Geometry
Group(VGG, http://www.robots.ox.ac.uk/~vgg/software/enceval_toolkit/)
is used, which eventually call VLFeat(http://www.vlfeat.org/). In my
code, 256 Gaussians were used.</p>

<h3 id="svm-classification">SVM Classification</h3>

<p>Binary SVM classification(<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">LIBSVM</a>) is used in my implementation.
For each action, positive video clips are labeled as 1 while negative
videos are as labeled -1 during training and test. In my code, SVM cost
is set to 100. The option of SVM training is:</p>

<blockquote>
  <p>-t 0 -s 0 -q -c 100 -b 1.</p>
</blockquote>

<h3 id="results">Results</h3>

<p>The action recognition accuracy of all the 101 actions was 77.95% when
using above pipeline. And the confusion matrix is shown in Figure 3.</p>

<p><img src="/assets/images/confusionmat_101.png" alt="Figure 3. Confusion matrix of all the 101 actions with STIP features." /></p>

<p>The mean accurary of the fist 10 actions with DTF features was 90.6%, while the STIP was only 84.32%. The mean accuracy of the whole UCF 101 data(train/test list 1) was around 85%, about 8% higher than using BOV representation(internal test). And the best result I got with the ISA network on UCF 101 was only 58% last year.</p>

<h3 id="conclusion">Conclusion</h3>

<p>It is obvious that Fisher vector can lead to better results than Bag-of-visual words in action recognition. Compared to other low-level visual features, DTF features have more advantages in action recognition. However, in the long run I still believe deep learning methods - when deep neural networks could be trained with millions of vidios[5], they would learn more info from scratch and achieve state-of-the-art accuracy.</p>

<h3 id="references">References</h3>

<ol>
  <li>Gabriela Csurka, Florent Perronnin, <em>Fisher Vectors: Beyond Bag-of-Visual-Words Image Representations</em> , Communications in Computer and Information Science Volume 229, 2011, pp 28-42.</li>
  <li>Chih-Chung Chang and Chih-Jen Lin. <em>Libsvm: A library for support vector machines</em>. ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27, May 2011.</li>
  <li>Heng Wang and Cordelia Schmid. <em>Action Recognition with Improved Trajectories</em>. In ICCV 2013 - IEEE International Conference on Computer Vision, Sydney, Australia, December 2013. IEEE.</li>
  <li>Jorge Sanchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. <em>Image Classification with the Fisher Vector: Theory and Practice</em>. International Journal of Computer Vision, 105(3):222–245, December 2013.</li>
  <li>Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. <em>Large-scale video classification with convolutional neural networks</em>. In CVPR, 2014.</li>
</ol>


	</div>

	<div align="right">
        <div class="bshare-custom">
	<span class='st_facebook_large' displayText='Facebook'></span>
	<span class='st_googleplus_large' displayText='Google +'></span>
	<span class='st_twitter_large' displayText='Tweet'></span>
	<span class='st_sina_large' displayText='Sina'></span>
	<span class='st_linkedin_large' displayText='LinkedIn'></span>
	<span class='st_tumblr_large' displayText='Tumblr'></span>
	<span class='st_sharethis_large' displayText='ShareThis'></span>
	<span class='st_blogger_large' displayText='Blogger'></span>
	<span class='st_email_large' displayText='Email'></span>
</div>
<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
<script type="text/javascript">stLight.options({publisher: "6e9cb968-ad78-4055-b1e7-1b7ab6751416", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script>

	</div>

	<!-- page link -->
    <div class="pager">
      <ul>
      
        <li class="previous"><a href="/2014/04/18/inclusion-property" title="Implementing Inclusion Property with SimpleScalar">&larr; Previous</a></li>
      

      
        <li class="next"><a href="/2014/05/26/binary-tree-traversal" title="Binary Tree Operations(I)">Next &rarr;</a></li>
      
      </ul>
    </div>

	<!--
	<div class="well">
		<script>
var linkwithin_site_id = 2140443;
</script>
<script src="http://www.linkwithin.com/widget.js"></script>
<a href="http://www.linkwithin.com/"><img src="http://www.linkwithin.com/pixel.png" alt="Related Posts Plugin for WordPress, Blogger..." style="border: 0" /></a>

	</div>
	-->
    <div class="well">
		


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'boyang'; // required: replace example with your forum shortname
     var disqus_identifier = '/2014/04/30/fisher-vector-in-action-recognition';
    var disqus_url = 'http://bo-yang.github.com//2014/04/30/fisher-vector-in-action-recognition';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




    </div>
</div>

<div class="col-lg-2">
	<div class="well">
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- RIght-side Ad(160x600) -->
<ins class="adsbygoogle"
     style="display:inline-block;width:160px;height:600px"
     data-ad-client="ca-pub-1292710832620237"
     data-ad-slot="3290428906"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

	</div>
	<div class="well">
		<h4>Visitors</h4>
		<div class="tag_box">
			<script type="text/javascript" src="http://je.revolvermaps.com/2/2.js?i=493pvkdo4nt&amp;m=0&amp;s=130&amp;c=ff0000&amp;t=1" async="async"></script>
	
		</div>
    </div>

</div>




    </div>

    <footer>
        <p>&copy; 2007 <span id="now_year"></span> Bo Yang. </p>
        <script type="text/javascript">
          var now_year = new Date().getFullYear();
          if (now_year != 2007) {
              $('#now_year').html('- ' + now_year);
          }
	    </script>
      </footer>
  </div>

  <script type="text/javascript" src="/assets/google-code-prettify/prettify.js"></script>
  <script src="/assets/run_prettify.js"></script>
  <script type="text/javascript">
    $(function() {

      $('a[href^="http"]').each(function () {
        $(this).attr('target', '_blank');
      });

      $('pre').addClass('prettyprint linenums').attr('style', 'overflow:auto');
      window.prettyPrint && prettyPrint();

      function traverse($node, len, maxCount) {
        var reachMaxCount = len > maxCount;
        if (reachMaxCount) {
          $node.hide();
        }
        var $contents = $node.contents();
        for (var i = 0; i < $contents.length; ++i) {
          if (reachMaxCount) {
            $contents.eq(i).hide();
            continue;
          }
          if ($contents[i].nodeType == 3) { // TextNode
            var tmp = len;
            var s = $contents[i].nodeValue;
            len += s.length;
            reachMaxCount = len > maxCount;
            if (reachMaxCount && $contents[i].parentNode.nodeName != 'A') {
              $contents[i].nodeValue = s.substring(0, maxCount - tmp);
            }
          }
          else if ($contents[i].nodeType == 1) { // Element
            len = traverse($contents.eq(i), len, maxCount);
          }
        }
        return len;
      }

      $('.post_at_index').each(function() {
        var count = traverse($(this), 0, 400);
        if (count > 400) {
          var thisUrl = $(this).siblings().first().children().attr('href');
          $(this).after('\n<a href="' + thisUrl + '" rel="nofollow" class="btn btn-primary" role="button">' + 'Read More &raquo;</a>');
        }
      });
    });
  </script>

  

</body>
</html>

