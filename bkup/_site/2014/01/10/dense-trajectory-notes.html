<p><span style="font-size:12px;font-weight:bold;line-height:1em;">Notes of Dense Trajectories</span><img class="aligncenter" style="font-size:12px;font-weight:bold;line-height:1em;" alt="" src="https://lh6.googleusercontent.com/AysSQ7M6yvc5hbkBHzsEHc1c7L4ZJcIXmdSfzh9g4iBDsL_q9lZD_8fhH8TEPMwAJ12Kp_s4z1FWmX3rndc6DTFAwARyWkx-mJiixB4X7nQRV5XcB391xltefQ" width="568px;" height="178px;" /></p>
<ul>
<li>densely sample feature points in each frame</li>
<li>track points in the video based on optical flow.</li>
<li>compute multiple descriptors along the trajectories of feature points to capture shape, appearance and motion information.</li>
</ul>
<ul>
<li>
<h3>Dense Sampling</h3>
<ul>
<li>Sampling step size W=5 pixels</li>
<li># spatial scales ≤ 8</li>
<li>Spatial scale increase: \( 1/\sqrt{2} \)</li>
<li>Removing points in homogeneous areas: \[ \arg\max \] <span style="font-style:inherit;font-weight:inherit;line-height:1.625;">where are eigenvalues of point i in image I (the auto-correlation matrix).</span></li>
</ul>
</li>
</ul>
<ul>
<li>
<h3>Descriptors</h3>
<ul>
<li>
<p dir="ltr">Trajectory shape descriptor(TR):</p>
</li>
</ul>
</li>
</ul>
<p dir="ltr"><img alt="" src="https://lh3.googleusercontent.com/TZG6Zx9WlI95UsEDsemdeXUHBtPrV6gJh_50zZzKJYTV45heKGfIx7xjlN079nJUZWMb93j9Vr72R0cvPQtdyVb49vDDeFpcLhcjeVq_VL8JhSh2VlZwtvNnxw" width="206px;" height="61px;" /></p>
<p dir="ltr">where L is the length of trajectory, and the displacement vectors <img alt="" src="https://lh3.googleusercontent.com/DqC7Y6vrH4mfXOVQQ29w8S9KH4jNH6kPpmyViWd5AhXhpVebHPO5xMTIdyWVAOPMvFU379KWsBR5FjV6cER0-whR6TxlBpch33vMqEVDXwC80uon5hQJMR3cxg" width="422px;" height="27px;" /></p>
<ul>
<li>HOG – static appearance information</li>
<li>HOF – local motion information</li>
<li>MBH – motion descriptor for trajectories</li>
</ul>
<ul>
<li>
<h3>Format of DTF features</h3>
</li>
</ul>
<h3>The format of the computed features</h3>
<p dir="ltr">The features are computed one by one, and each one in a single line, with the following format:</p>
<p dir="ltr">frameNum mean_x mean_y var_x var_y length scale x_pos y_pos t_pos Trajectory HOG HOF MBHx MBHy</p>
<p dir="ltr">The first 10 elements are information about the trajectory:</p>
<ul>
<li>frameNum:     The trajectory ends on which frame</li>
<li>mean_x:       The mean value of the x coordinates of the trajectory</li>
<li>mean_y:       The mean value of the y coordinates of the trajectory</li>
<li>var_x:        The variance of the x coordinates of the trajectory</li>
<li>var_y:        The variance of the y coordinates of the trajectory</li>
<li>length:       The length of the trajectory</li>
<li>scale:        The trajectory is computed on which scale</li>
<li>x_pos:        The normalized x position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
<li>y_pos:        The normalized y position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
<li>t_pos:        The normalized t position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
</ul>
<p dir="ltr">The following element are five descriptors concatenated one by one:</p>
<ul>
<li>Trajectory:    2x[trajectory length] (default 30 dimension)</li>
<li>HOG:           8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
<li>HOF:           9x[spatial cells]x[spatial cells]x[temporal cells] (default 108 dimension)</li>
<li>MBHx:          8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
<li>MBHy:          8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
</ul>
<ol start="2">
<li>
<h2>Improved Dense Trajectories</h2>
</li>
</ol>
<ul>
<li>Explicit camera motion estimation</li>
<li>Assumption: two consecutive frames are related by a homography.</li>
<li>Match feature points between frames using SURF descriptors and dense optical flow</li>
<li>Removing inconsistent matches due to humans: use a human detector to remove matches from human regions (computation expensive)</li>
<li>Estimate a homography with RANSAC with these matches</li>
</ul>
<h2>References:</h2>
<ol>
<li>
<div>H Wang, C Schmid, Action recognition with improved trajectories, ICCV 2013</div>
</li>
<li>
<div>H Wang, A Kläser, C Schmid, CL Liu, Dense trajectories and motion boundary descriptors for action recognition, International Journal of Computer Vision, May 2013, Volume 103, Issue 1, pp 60-79</div>
</li>
</ol>
